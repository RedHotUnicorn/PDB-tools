from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import pandas as pd
import spacy
from markdown_plain_text.extention import convert_to_plain_text  # https://github.com/kostyachum/python-markdown-plain-text
import re
import emoji
import _utils as u
import sqlite3

import string



pd.options.display.width = 0
pd.options.display.width = 1500
pd.set_option('display.max_columns', None)
pd.set_option('display.expand_frame_repr', False)
pd.set_option('max_colwidth', None)

nlp = spacy.load("ru_core_news_lg")





text = """*–ï—â—ë* –æ–¥–Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –ø–æ–ø–∞–ª–∞—Å—å.
# –ê–≤—Ç–æ—Ä - Koen van den Eeckhout.
–ò–¥–µ—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è, –Ω–æ –≤—Å–µ –∂–µ, –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ. –ù–∞–≤–µ—Ä–Ω–æ–µ, –æ–±–ª–∞–∫–∞ –∏–ª–∏ –¥–∏–∞–≥—Ä–∞–º–º–∞ –≤–µ–Ω–Ω–∞ –±—ã–ª–∞ –±—ã —Ç—É—Ç —É–º–µ—Å—Ç–Ω–µ–µ.
–ù–∞–ø—Ä–∏–º–µ—Ä, —á—Ç–æ –¥–µ–ª–∞—Ç—å —Å –ø–µ—á–∞—Ç–Ω—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏? –ö–æ–≤–∞—Ä–Ω—ã–µ –∫–∞—Ä—Ç—ã –±—ã–≤–∞—é—Ç —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –≤—Å–µ—Ö —Ç—Ä–µ—Ö –≤–∏–¥–æ–≤
**üòÅ**
–ù–æ –¥–ª—è –ø–æ–¥—É–º–∞—Ç—å –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è, ~~—á—Ç–æ–±—ã~~ –¥–ª—è —Å–µ–±—è –Ω–∞ —Å—Ç–∞—Ä—Ç–µ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã:
- [test](https://docs.python.org/3/tutorial/datastructures.html)
- https://docs.python.org/3/tutorial/datastructures.html
- –ê –∫–∞–∫–æ–π –∂–µ –ø—Ä–æ–¥—É–∫—Ç –º—ã —Ö–æ—Ç–∏–º?
- –ö–∞–∫ –±—ã—Å—Ç—Ä–æ –µ–≥–æ –¥–æ–ª–∂–µ–Ω –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å —á–∏—Ç–∞—Ç–µ–ª—å?
- –ö–∞–∫ –∏ –∫—Ç–æ –±—É–¥–µ—Ç —Å –Ω–∏–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å?
–ò—Å—Ç–æ—á–Ω–∏–∫ —Ç—É—Ç (–≤–ø–Ω):
> quotes
__This is bold text__
"""


text = """
–°—Ç–∞—Ç–µ–π –æ —Ä–∞–±–æ—Ç–µ —Å PostgreSQL –∏ –µ—ë –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–Ω–æ–≥–æ, –Ω–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –∏–∑ –Ω–∏—Ö –ø–æ–Ω—è—Ç–Ω–æ, –∫–∞–∫ —Å–ª–µ–¥–∏—Ç—å –∑–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –±–∞–∑—ã –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏, –≤–ª–∏—è—é—â–∏–º–∏ –Ω–∞ –µ—ë –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Ä–∞–±–æ—Ç—É. –í —Å—Ç–∞—Ç—å–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º SQL-–∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥—É—Ç –≤–∞–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —ç—Ç–∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∏ –ø—Ä–æ—Å—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
## –ó–∞—á–µ–º —Å–ª–µ–¥–∏—Ç—å –∑–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º PostgreSQL?
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Ç–∞–∫–∂–µ –≤–∞–∂–µ–Ω, –∫–∞–∫ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤–∞—à–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ, —á–µ–º –Ω–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–º —É—Ä–æ–≤–Ω–µ. –î–ª—è —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:
–ù–∞—Å–∫–æ–ª—å–∫–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –∫—ç—à –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö?
–ö–∞–∫–æ–π —Ä–∞–∑–º–µ—Ä —Ç–∞–±–ª–∏—Ü –≤ –≤–∞—à–µ–π –ë–î?
–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ª–∏ –≤–∞—à–∏ –∏–Ω–¥–µ–∫—Å—ã?
–ò —Ç–∞–∫ –¥–∞–ª–µ–µ.
## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–∑–º–µ—Ä–∞ –ë–î –∏ –µ—ë —ç–ª–µ–º–µ–Ω—Ç–æ–≤
### 1. –†–∞–∑–º–µ—Ä —Ç–∞–±–ª–∏—á–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤

```
SELECT spcname, pg_size_pretty(pg_tablespace_size(spcname))
FROM pg_tablespace
WHERE spcname<>'pg_global';
```
–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–µ –≤—Å–µ—Ö tablespace —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –≤ –≤–∞—à–µ–π –ë–î. –§—É–Ω–∫—Ü–∏—è **pg_tablespace_size** –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–µ tablespace –≤ –±–∞–π—Ç–∞—Ö, –ø–æ—ç—Ç–æ–º—É –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ —á–∏—Ç–∞–µ–º–æ–º—É –≤–∏–¥—É –º—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é **pg_size_pretty**. –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ pg_global –∏—Å–∫–ª—é—á–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—â–∏—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∫–∞—Ç–∞–ª–æ–≥–æ–≤.
### 2. –†–∞–∑–º–µ—Ä –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö

```
SELECT pg_database.datname,
pg_size_pretty(pg_database_size(pg_database.datname)) AS size
FROM pg_database
ORDER BY pg_database_size(pg_database.datname) DESC;
```
–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–µ –≤—Å–µ—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–º–∫–∞—Ö –≤–∞—à–µ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ PostgreSQL.

"""



data = {

"id": [1] ,
"markdown" : [text]
}



data = pd.read_sql_query (f"""
                              SELECT
                                          id
                                        , markdown
                              FROM     content
                              WHERE    m_length   > 100 
                                   and markdown not like '%00:00%' 
                                   and id             in (select id from NTN_V_url where  url like '%t.me%')       
                              LIMIT 10  
                              """, sqlite3.connect(r'') )




def clean_text(rubbish_text):
    text_wo_emoji   = emoji.replace_emoji(rubbish_text, '')
    text_wo_code    = re.sub(r"```[^\S\r\n]*[a-z]*\n.*?\n```", ' INNER_CODE_BLOCK.', text_wo_emoji, 0, re.DOTALL)
    text_wo_md      = convert_to_plain_text(text_wo_code)                                                                # pd.DataFrame(data, columns = ['id', 'markdown']).replace(r"\[(.+)\]\(.+\)", '', regex=True).replace(r'http\S+', '', regex=True)  
    text_wo_links   = re.sub('http\S+','' ,text_wo_md)

    text_wo_uchars  = re.sub("[^\w.,!?\n]", " ", text_wo_links)     
    
    text_wo_dblspc  = re.sub(' {2,}', ' ', text_wo_uchars)                                                                 # https://stackoverflow.com/a/71340209/5353177

    lines           = text_wo_dblspc.split("\n") 
 
    # Add dot in the end of sent
    for i, l in enumerate(lines):
        if not l.endswith(tuple(string.punctuation)):
            lines[i] = l.strip() + '.'

    # Filter sent if it's not contain words
    lines           = [l for l in lines if l.strip(string.punctuation).replace(" ", "") != '']

    text_w_punct    = ' '.join(lines)

    # print(text_w_punct)
    # print('-'* 400)

    text            = text_w_punct
    return text




# Explode markdown to sentences
def expl_sent(text,num_of_sent = 3):
    with nlp.select_pipes(enable=['tok2vec', "parser", "senter"]):
          doc = nlp(text)
    sent = [str(sentence) for sentence in doc.sents  ]  

    
    # sent3= list(zip( sent[0:],sent[1:],sent[2:] ))
    # return [" ".join(map(str,item)) for item in sent3]  

    zip_arg = []
    for i in range(0,num_of_sent ):
        if len(sent) > i:
            zip_arg.append(sent[i:]) 
    sentN= list(zip( *zip_arg )) 

    return [" ".join(map(str,item)) for item in sentN]     



df_docs             = pd.DataFrame(data, columns = ['id', 'markdown'])
df_docs['sent']     = df_docs['markdown'].apply(clean_text).apply(expl_sent)
df_docs             = df_docs.explode('sent',ignore_index=True)

# print(df_docs['sent'])






def lemmatize(text: str):
     doc = nlp(text.lower())
     lemmas = []
     for token in doc:
          lemmas.append(token.lemma_)
     return lemmas


ctfidf_model        = ClassTfidfTransformer(reduce_frequent_words=True)
my_stopwords        = stopwords.words("russian")
vectorizer_model    = CountVectorizer(stop_words=my_stopwords,tokenizer=lemmatize)

topic_model         = BERTopic(  language           = "multilingual"
                                ,ctfidf_model       = ctfidf_model
                                ,vectorizer_model   = vectorizer_model
                                )
topics, probs       = topic_model.fit_transform(df_docs['sent'])
# print(topic_model.get_topic_info(df_docs['sent']))

res                 = topic_model.get_document_info(df_docs['sent'])
df_topics           = pd.DataFrame(topic_model.get_topics().items(), columns=['Topic', 'Words']) 
print(df_topics)

# print(res)
# res.to_excel(r'out/BERTopic.xlsx')

df_joined           = df_docs[['id']].join(res[['Topic' , 'Name','Representation']].query('Topic >= 0'))
df_joined['Topic']  = df_joined['Topic'].fillna(-1.0).astype(int)
df_joined           = df_joined.query('Topic >= 0')



gr_id               = df_joined.groupby(['id'])
gr_id_topic         = df_joined.groupby(['id','Name','Topic'])

count_by_id_topic   = gr_id_topic.agg({'Topic': 'count'}).rename(columns={'Topic': 'count_by_id_topic'})
count_by_id         = gr_id.size().to_frame(name = 'count_by_id')                                                                                               
count_unique_by_id  = gr_id.agg({'Topic': 'nunique'}).rename(columns={'Topic': 'count_unique_by_id'})

res_stat            = (  count_by_id 
                        .join(count_by_id_topic)
                        .join(count_unique_by_id)
                        # .join(df_topics.set_index('Topic') ,on = 'Topic',)
                      )   
res_stat['perc_count'] = res_stat['count_by_id_topic'] / res_stat['count_by_id']
print(res_stat)


res_stat            = res_stat.query('count_unique_by_id < 3 or perc_count > 0.1')
print(res_stat)



