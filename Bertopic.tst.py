from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import PDButils as u
import pandas as pd
import spacy
from markdown_plain_text.extention import convert_to_plain_text

nlp = spacy.load("ru_core_news_lg")

text = """Ð•Ñ‰Ñ‘ Ð¾Ð´Ð½Ð° Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð°Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¹ Ð¿Ð¾Ð¿Ð°Ð»Ð°ÑÑŒ.
ÐÐ²Ñ‚Ð¾Ñ€ - Koen van den Eeckhout.
Ð˜Ð´ÐµÑ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð°Ñ, Ð½Ð¾ Ð²ÑÐµ Ð¶Ðµ, Ð½Ðµ Ð¸Ð´ÐµÐ°Ð»ÑŒÐ½Ð¾. ÐÐ°Ð²ÐµÑ€Ð½Ð¾Ðµ, Ð¾Ð±Ð»Ð°ÐºÐ° Ð¸Ð»Ð¸ Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ð° Ð²ÐµÐ½Ð½Ð° Ð±Ñ‹Ð»Ð° Ð±Ñ‹ Ñ‚ÑƒÑ‚ ÑƒÐ¼ÐµÑÐ¸Ð½ÐµÐµ.
ÐÐ°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ñ‡Ñ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ñ Ð¿ÐµÑ‡Ð°Ñ‚Ð½Ñ‹Ð¼Ð¸ ÐºÐ°Ñ€Ñ‚Ð°Ð¼Ð¸? ÐšÐ¾Ð²Ð°Ñ€Ð½Ñ‹Ðµ ÐºÐ°Ñ€Ñ‚Ñ‹ Ð±Ñ‹Ð²Ð°ÑŽÑ‚ ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½Ð½Ð¾ Ð²ÑÐµÑ… Ñ‚Ñ€ÐµÑ… Ð²Ð¸Ð´Ð¾Ð²
**ðŸ˜**
ÐÐ¾ Ð´Ð»Ñ Ð¿Ð¾Ð´ÑƒÐ¼Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¸Ð³Ð¾Ð´Ð¸Ñ‚ÑÑ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð´Ð»Ñ ÑÐµÐ±Ñ Ð½Ð° ÑÑ‚Ð°Ñ€Ñ‚Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹:
- Ð ÐºÐ°ÐºÐ¾Ð¹ Ð¶Ðµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚ Ð¼Ñ‹ Ñ…Ð¾Ñ‚Ð¸Ð¼?
- ÐšÐ°Ðº Ð±Ñ‹ÑÑ‚Ñ€Ð¾ ÐµÐ³Ð¾ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð²Ð¾ÑÐ¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ñ‡Ð¸Ñ‚Ð°Ñ‚ÐµÐ»ÑŒ?
- ÐšÐ°Ðº Ð¸ ÐºÑ‚Ð¾ Ð±ÑƒÐ´ÐµÑ‚ Ñ Ð½Ð¸Ð¼ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ?
Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ñ‚ÑƒÑ‚ (Ð²Ð¿Ð½):
"""

# only select necessary pipeline components to speed up processing

with nlp.select_pipes(enable=['tok2vec', "parser", "senter"]):
    doc = nlp(text)
    
for sentence in doc.sents:
    print("-"*100)
    print(sentence)

def expl_sent(text):
     with nlp.select_pipes(enable=['tok2vec', "parser", "senter"]):
          doc = nlp(text)
     return  [str(sentence) for sentence in doc.sents  ]

sql_query = pd.read_sql_query (f"""
                                                               SELECT
                                    id
                                    ,markdown
                               FROM content
                               where 
                                    m_length > 100 
                                 and markdown not like '%00:00%' 
                                -- and markdown not like '% Ð¸ %' 
                                 --and markdown not like '% Ð½Ðµ %' 
                                 and id in (select id
                                                    from NTN_V_url where  url like '%t.me%') 

                                --limit 500              
                               """, u.DB_CONNECTION)

df_docs = pd.DataFrame(sql_query, columns = ['id', 'markdown']).replace(r"\[(.+)\]\(.+\)", '', regex=True).replace(r'http\S+', '', regex=True)
df_docs['sent'] = df_docs['markdown'].apply(convert_to_plain_text).apply(expl_sent)
df_docs=df_docs.explode('sent')

print(df_docs)


ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)
my_stopwords = stopwords.words("russian")
vectorizer_model = CountVectorizer(stop_words=my_stopwords)

topic_model = BERTopic(language="multilingual"
                       ,ctfidf_model=ctfidf_model
                         ,vectorizer_model=vectorizer_model
                       )
topics, probs = topic_model.fit_transform(df_docs['sent'])
# print(topic_model.get_topic_info(df_docs['sent']))

topic_model.get_document_info(df_docs['sent']).to_excel(r'BERTopic.xlsx')

